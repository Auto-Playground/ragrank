```{toctree}
:hidden:

get_started/index
core_concepts/index
evaluation/index
metrics/index
integrations/index
api_reference/index
community/index
more/index
```

# Ragrank ğŸ¯

Ragrank is a user-friendly Python library created to make evaluating Retrieval Augmented Generation (RAG) models easier ğŸ”¥.


**With Ragrank, you can:**

- Evaluate, monitor, and troubleshoot LLM applications ğŸ› 
- Plug-and-use 5+ LLM-evaluated metrics, most with research backing ğŸ’¡
- Custom metrics are simple to personalize and create ğŸ“
- Define evaluation datasets in Python code ğŸ“¦



**Key Features:**

- Easy to Use: Designed for developers new to RAG models ğŸ“˜
- Customizable: Tailor the evaluation process to your data and metrics ğŸ› ï¸
- Open Source: The source code is publically available ğŸŒŸ
-------


````{grid}
:gutter: 2

```{grid-item-card} ğŸš€ Quick Start
:link: quick-start
:link-type: ref

Master the basics of `ragrank` with a strong step. Evaluate RAG pipelines, generate test sets, and set up online monitoring for RAG apps â€” all with a few lines of code.
```

```{grid-item-card} ğŸª© Core concepts
:link: core-concepts
:link-type: ref

Discover the main ideas and concepts of `ragrank`. Learn how to check how well models work using the tools in the library with fundamentals.
```

````

````{grid}
:gutter: 2

```{grid-item-card} ğŸ›  The Evaluation
:link: the-evaluation
:link-type: ref

Head over to the comprehensive evaluation with `ragrank`. Explore the various aspects of evaluating Language Model (LLM) and Retrieval Augmented Generation (RAG) models.
```

```{grid-item-card} ğŸ“ Metrics
:link: the-metrics
:link-type: ref

Discover different metrics and create custom ones using `ragrank`. Dive into a variety of measurement methods to analyze and improve the performance of your models effectively.
```

````
